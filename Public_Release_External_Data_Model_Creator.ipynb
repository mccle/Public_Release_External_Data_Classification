{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Functions (Run Every Time)\n",
    "\n",
    "The following block of code consists of all of the imports and functions required to run the code in all of the sections below. Thus, this block must be run every time the kernel is restarted (important to run when first opening the document and when using Section 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:30:01.853185Z",
     "start_time": "2022-04-27T08:29:58.083929Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "### Imports\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['axes.labelsize'] = '16'\n",
    "rcParams['legend.frameon'] = False\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.colors import LogNorm\n",
    "import os\n",
    "from astropy.table import Table\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "### Underlying Functions\n",
    "\n",
    "def random_uniform_array(start,stop,size):\n",
    "    values=np.zeros(size)\n",
    "    for i in range(size):\n",
    "        values[i]=random.uniform(start,stop)\n",
    "    return values\n",
    "\n",
    "def Rotation_Matrix(alpha,beta,gamma): ### z, y, x / yaw, pitch, roll\n",
    "    return np.array([[np.cos(alpha)*np.cos(beta),np.cos(alpha)*np.sin(beta)*np.sin(gamma)-np.sin(alpha)*np.cos(gamma),np.cos(alpha)*np.sin(beta)*np.cos(gamma)+np.sin(alpha)*np.sin(gamma)],\n",
    "                     [np.sin(alpha)*np.cos(beta),np.sin(alpha)*np.sin(beta)*np.sin(gamma)+np.cos(alpha)*np.cos(gamma),np.sin(alpha)*np.sin(beta)*np.cos(gamma)-np.cos(alpha)*np.sin(gamma)],\n",
    "                     [-np.sin(beta),np.cos(beta)*np.sin(gamma),np.cos(beta)*np.cos(gamma)]])\n",
    "\n",
    "def Projected_Points(points,alpha,beta,gamma):\n",
    "    projections=np.zeros(points.shape)\n",
    "    R=Rotation_Matrix(alpha,beta,gamma)\n",
    "    for i in range(len(points)):\n",
    "        v=points[i]\n",
    "        projections[i]=(R@v)\n",
    "    return projections\n",
    "\n",
    "def Tensor_Image(points):\n",
    "    plt.rcParams[\"figure.figsize\"]=(25/36,25/36)\n",
    "    fig=plt.figure()\n",
    "    plot=fig.add_subplot(111)\n",
    "    nan_indices = np.isnan(points[:,0]) | np.isnan(points[:,1])\n",
    "    finite_indices=~nan_indices\n",
    "    plot.hist2d(points[:,0][finite_indices],points[:,1][finite_indices],bins=(50,50),norm=LogNorm())\n",
    "    plot.axis(\"equal\")\n",
    "    plot.axis(\"off\")\n",
    "    fig.canvas.draw()\n",
    "    w,h=fig.canvas.get_width_height()\n",
    "    Image=np.fromstring(fig.canvas.tostring_argb(),dtype=np.uint8)\n",
    "    Image.shape=(w,h,4)\n",
    "    Image=np.roll(Image,3,axis=2)\n",
    "    Tensor_Image=tf.cast(Image,tf.float32)/255\n",
    "    plt.close()\n",
    "    return Tensor_Image\n",
    "\n",
    "def Save_Image_Set(Images,File_Name):\n",
    "    if type(Images)==tf.Tensor:\n",
    "        Images=Images.numpy()\n",
    "        np.save(File_Name,Images)\n",
    "    else:\n",
    "        np.save(File_Name,Images)\n",
    "    \n",
    "def Load_Array(File_Name):\n",
    "    Images=np.load(File_Name,allow_pickle=True)        \n",
    "    if type(Images[0])==np.ndarray:\n",
    "        Images=tf.cast(Images,tf.float32)\n",
    "    else:\n",
    "        Images=tf.cast(Images,tf.string)\n",
    "    return Images\n",
    "\n",
    "def Image_Set_Classifications(N1,File_Name,classification):\n",
    "    Class_Name=File_Name[:-4]+\"_classifications.npy\"\n",
    "    classifications=np.repeat(classification,N1)\n",
    "    np.save(Class_Name,classifications)\n",
    "    \n",
    "def Classifications_to_Labels(Data_Classifications,class_names):\n",
    "    Labels=np.ones(len(Data_Classifications))\n",
    "    for i in range(len(Data_Classifications)):\n",
    "        for j in range(len(class_names)):\n",
    "            if Data_Classifications[i]==class_names[j]:\n",
    "                Labels[i]=j\n",
    "    return Labels\n",
    "\n",
    "def Image_Reader(Data_Folder):\n",
    "    cwd=os.getcwd()\n",
    "    os.chdir(Data_Folder)\n",
    "    Files=glob.glob(\"*.fits\")\n",
    "    \n",
    "    File_name_0=Files[0]\n",
    "    alldata_0=Table.read(File_name_0)\n",
    "    xdata_0=alldata_0[\"pos_x\"]\n",
    "    ydata_0=alldata_0[\"pos_y\"]\n",
    "    zdata_0=alldata_0[\"pos_z\"]\n",
    "    \n",
    "    points=np.empty((len(xdata_0),3))\n",
    "    for j in range(len(xdata_0)):\n",
    "        points[j]=np.array([xdata_0[j],ydata_0[j],zdata_0[j]])\n",
    "        \n",
    "    Data=[Tensor_Image(points)]\n",
    "    \n",
    "    for i in range(1,len(Files)):\n",
    "        File_name=Files[i]\n",
    "        alldata=Table.read(File_name)\n",
    "        xdata=alldata[\"pos_x\"]\n",
    "        ydata=alldata[\"pos_y\"]\n",
    "        zdata=alldata[\"pos_z\"]\n",
    "        \n",
    "        points=np.empty((len(xdata),3))\n",
    "        for j in range(len(xdata)):\n",
    "            points[j]=np.array([xdata[j],ydata[j],zdata[j]])\n",
    "        New_Data=Tensor_Image(points)\n",
    "        \n",
    "        Data.append(New_Data)\n",
    "    os.chdir(cwd)\n",
    "    \n",
    "    Data=tf.cast(Data,tf.float32)\n",
    "    return Files,Data\n",
    "\n",
    "def Import_Sheet_Classifications(Sheet_ID,Sheet_Names):\n",
    "    URL=f\"https://docs.google.com/spreadsheets/d/{Sheet_ID}/gviz/tq?tqx=out:csv&sheet={Sheet_Names[0]}\"\n",
    "    Sheet=pd.read_csv(URL,warn_bad_lines=False)\n",
    "    Files=np.array(Sheet[\"Halo ID\"],dtype=str)\n",
    "    Classifications=np.array(Sheet[\"Classification\"],dtype=str)\n",
    "    for i in range(1,len(Sheet_Names)):\n",
    "        URL=f\"https://docs.google.com/spreadsheets/d/{Sheet_ID}/gviz/tq?tqx=out:csv&sheet={Sheet_Names[i]}\"\n",
    "        Sheet=pd.read_csv(URL,warn_bad_lines=False)\n",
    "        New_Files=np.array(Sheet[\"Halo ID\"],dtype=str)\n",
    "        New_Classifications=np.array(Sheet[\"Classification\"],dtype=str)\n",
    "        Files=np.append(Files,New_Files)\n",
    "        Classifications=np.append(Classifications,New_Classifications)\n",
    "    return Files,Classifications\n",
    "\n",
    "def Import_Folder_Files(Folder):\n",
    "    Files=[]\n",
    "    for i in glob.glob(Folder+\"/**/*.fits\",recursive=True):\n",
    "        Files.append(str(i))\n",
    "    Files=np.array(Files,dtype=str)\n",
    "    return Files\n",
    "\n",
    "def Large_Set_Reader(Prediction_Folder):\n",
    "    cwd=os.getcwd()\n",
    "    os.chdir(Prediction_Folder)\n",
    "    Folders=glob.glob(\"*.fits\")\n",
    "    Files=[]\n",
    "    Tensor_Images=[]\n",
    "    Classifications=[]\n",
    "    for Folder in Folders:\n",
    "        Files.append(Folder)\n",
    "        os.chdir(Folder)\n",
    "        Tensor_Image=np.load(\"Tensor_Image.npy\")\n",
    "        Tensor_Images.append(Tensor_Image)\n",
    "        Classification=np.load(\"Classification.npy\")\n",
    "        Classifications.append(Classification[0])\n",
    "        os.chdir(\"..\")\n",
    "    os.chdir(cwd)\n",
    "    \n",
    "    Files=np.array(Files)\n",
    "    Tensor_Images=np.array(Tensor_Images)\n",
    "    Classifications=np.array(Classifications)\n",
    "    \n",
    "    return Files,Tensor_Images,Classifications\n",
    "\n",
    "### Primary Functions\n",
    "\n",
    "def Sheet_Data_Set_Creator(Folder,Sheet_ID,Sheet_Names,Set_Name):\n",
    "    Sheet_Files,Sheet_Classifications=Import_Sheet_Classifications(Sheet_ID,Sheet_Names)\n",
    "    Folder_Files=Import_Folder_Files(Folder)\n",
    "    Included_Files=[]\n",
    "    Included_Classifications=[]\n",
    "    Indices=[]\n",
    "    for i in range(len(Sheet_Files)):\n",
    "        if np.flatnonzero(np.char.find(Folder_Files,Sheet_Files[i])!=-1).size !=0 and Sheet_Classifications[i]!=\"nan\":\n",
    "            Indices.append(np.flatnonzero(np.char.find(Folder_Files,Sheet_Files[i])!=-1)[0])\n",
    "            Included_Files.append(Folder_Files[Indices[-1]])\n",
    "            Included_Classifications.append(Sheet_Classifications[i])\n",
    "    Included_Files=np.array(Included_Files)\n",
    "    \n",
    "    with alive_bar(len(Indices),force_tty=True) as bar:\n",
    "    \n",
    "        File_name_0=Folder_Files[Indices[0]]\n",
    "        alldata_0=Table.read(File_name_0)\n",
    "        xdata_0=alldata_0[\"pos_x\"]\n",
    "        ydata_0=alldata_0[\"pos_y\"]\n",
    "        zdata_0=alldata_0[\"pos_z\"]\n",
    "        points=np.empty((len(xdata_0),3))\n",
    "        for j in range(len(xdata_0)):\n",
    "            points[j]=np.array([xdata_0[j],ydata_0[j],zdata_0[j]])\n",
    "        Data=[Tensor_Image(points)]\n",
    "        Data=tf.cast(Data,tf.float32)\n",
    "        bar()\n",
    "    \n",
    "    \n",
    "        for i in range(1,len(Indices)):\n",
    "            File_name=Folder_Files[Indices[i]]\n",
    "            alldata=Table.read(File_name)\n",
    "            xdata=alldata[\"pos_x\"]\n",
    "            ydata=alldata[\"pos_y\"]\n",
    "            zdata=alldata[\"pos_z\"]\n",
    "            points=np.empty((len(xdata),3))\n",
    "            for j in range(len(xdata)):\n",
    "                points[j]=np.array([xdata[j],ydata[j],zdata[j]])\n",
    "            New_Data=[Tensor_Image(points)]\n",
    "            New_Data=tf.cast(New_Data,tf.float32)\n",
    "            Data=tf.concat((Data,New_Data),axis=0)\n",
    "            bar()\n",
    "        \n",
    "    Save_Image_Set(Included_Files,\"External_Data_Image_Sets/\"+Set_Name+\"_Files.npy\")\n",
    "    Save_Image_Set(Data,\"External_Data_Image_Sets/\"+Set_Name+\".npy\")\n",
    "    Save_Image_Set(Included_Classifications,\"External_Data_Image_Sets/\"+Set_Name+\"_Classifications.npy\")\n",
    "\n",
    "def nan_Set_Creator(Folder,Sheet_ID,Sheet_Names,Set_Name):\n",
    "    Sheet_Files,Sheet_Classifications=Import_Sheet_Classifications(Sheet_ID,Sheet_Names)\n",
    "    Folder_Files=Import_Folder_Files(Folder)\n",
    "    Included_Files=[]\n",
    "    Included_Classifications=[]\n",
    "    Indices=[]\n",
    "    for i in range(len(Sheet_Files)):\n",
    "        if  np.flatnonzero(np.char.find(Folder_Files,Sheet_Files[i])!=-1).size !=0 and Sheet_Classifications[i]==\"nan\":\n",
    "            Indices.append(np.flatnonzero(np.char.find(Folder_Files,Sheet_Files[i])!=-1)[0])\n",
    "            Included_Files.append(Folder_Files[Indices[-1]])\n",
    "            Included_Classifications.append(Sheet_Classifications[i])\n",
    "    Included_Files=np.array(Included_Files)\n",
    "    \n",
    "    with alive_bar(len(Indices),force_tty=True) as bar:\n",
    "        File_name_0=Folder_Files[Indices[0]]\n",
    "        alldata_0=Table.read(File_name_0)\n",
    "        xdata_0=alldata_0[\"pos_x\"]\n",
    "        ydata_0=alldata_0[\"pos_y\"]\n",
    "        zdata_0=alldata_0[\"pos_z\"]\n",
    "        points=np.empty((len(xdata_0),3))\n",
    "        for j in range(len(xdata_0)):\n",
    "            points[j]=np.array([xdata_0[j],ydata_0[j],zdata_0[j]])\n",
    "        Data=[Tensor_Image(points)]\n",
    "        Data=tf.cast(Data,tf.float32)\n",
    "        bar()\n",
    "    \n",
    "        for i in range(1,len(Indices)):\n",
    "            File_name=Folder_Files[Indices[i]]\n",
    "            alldata=Table.read(File_name)\n",
    "            xdata=alldata[\"pos_x\"]\n",
    "            ydata=alldata[\"pos_y\"]\n",
    "            zdata=alldata[\"pos_z\"]\n",
    "            points=np.empty((len(xdata),3))\n",
    "            for j in range(len(xdata)):\n",
    "                points[j]=np.array([xdata[j],ydata[j],zdata[j]])\n",
    "            New_Data=[Tensor_Image(points)]\n",
    "            New_Data=tf.cast(New_Data,tf.float32)\n",
    "            Data=tf.concat((Data,New_Data),axis=0)\n",
    "            bar()    \n",
    "    Save_Image_Set(Included_Files,\"External_Data_Image_Sets/\"+Set_Name+\"_Files.npy\")\n",
    "    Save_Image_Set(Data,\"External_Data_Image_Sets/\"+Set_Name+\".npy\")\n",
    "    Save_Image_Set(Included_Classifications,\"External_Data_Image_Sets/\"+Set_Name+\"_Classifications.npy\")\n",
    "\n",
    "def Data_Set_Extender(Current_Set_Name,Final_Set_Name):\n",
    "    Data=Load_Array(\"External_Data_Image_Sets/\"+Current_Set_Name+\".npy\")\n",
    "    Current_Classifications=Load_Array(\"External_Data_Image_Sets/\"+Current_Set_Name+\"_Classifications.npy\")\n",
    "    Files=Load_Array(\"External_Data_Image_Sets/\"+Current_Set_Name+\"_Files.npy\")\n",
    "    Classifications=Current_Classifications\n",
    "    \n",
    "    Single_Node_Condition=tf.where(Current_Classifications==\"Single Node\")\n",
    "    Single_Node_Indices=[]\n",
    "    for i in range(len(Single_Node_Condition)):\n",
    "        Single_Node_Indices.append(Single_Node_Condition[i][0].numpy())\n",
    "    \n",
    "    Multiple_Node_Condition=tf.where(Current_Classifications==\"Multiple Nodes\")\n",
    "    Multiple_Node_Indices=[]\n",
    "    for i in range(len(Multiple_Node_Condition)):\n",
    "        Multiple_Node_Indices.append(Multiple_Node_Condition[i][0].numpy())\n",
    "    N=np.abs(len(Multiple_Node_Indices)-len(Single_Node_Indices))\n",
    "    if len(Multiple_Node_Indices)>len(Single_Node_Indices):\n",
    "        with alive_bar(int(N),force_tty=True) as bar:\n",
    "            for i in range(N):\n",
    "                random_i=np.random.choice(Single_Node_Indices)\n",
    "                File_name=Files[random_i]\n",
    "                alldata=Table.read(File_name.numpy().decode(\"utf-8\"))\n",
    "                xdata=alldata[\"pos_x\"]\n",
    "                ydata=alldata[\"pos_y\"]\n",
    "                zdata=alldata[\"pos_z\"]\n",
    "                points=np.empty((len(xdata),3))\n",
    "                for j in range(len(xdata)):\n",
    "                    points[j]=np.array([xdata[j],ydata[j],zdata[j]])\n",
    "                alpha,beta,gamma=random_uniform_array(-2*np.pi,2*np.pi,3)\n",
    "                projections=Projected_Points(points,alpha,beta,gamma)\n",
    "                New_Data=[Tensor_Image(projections)]\n",
    "                New_Data=tf.cast(New_Data,tf.float32)\n",
    "                Data=tf.concat((Data,New_Data),axis=0)\n",
    "                New_Classification=tf.cast([Current_Classifications[random_i]],tf.string)\n",
    "                Classifications=tf.concat((Classifications,New_Classification),axis=0)\n",
    "                New_File=tf.cast([File_name],tf.string)\n",
    "                Files=tf.concat((Files,New_File),axis=0)\n",
    "                bar()\n",
    "    if len(Multiple_Node_Indices)<len(Single_Node_Indices):\n",
    "        with alive_bar(int(N),force_tty=True) as bar:\n",
    "            for i in range(N):\n",
    "                random_i=np.random.choice(Multiple_Node_Indices)\n",
    "                File_name=Files[random_i]\n",
    "                alldata=Table.read(File_name.numpy().decode(\"utf-8\"))\n",
    "                xdata=alldata[\"pos_x\"]\n",
    "                ydata=alldata[\"pos_y\"]\n",
    "                zdata=alldata[\"pos_z\"]\n",
    "                points=np.empty((len(xdata),3))\n",
    "                for j in range(len(xdata)):\n",
    "                    points[j]=np.array([xdata[j],ydata[j],zdata[j]])\n",
    "                alpha,beta,gamma=random_uniform_array(-2*np.pi,2*np.pi,3)\n",
    "                projections=Projected_Points(points,alpha,beta,gamma)\n",
    "                New_Data=[Tensor_Image(projections)]\n",
    "                New_Data=tf.cast(New_Data,tf.float32)\n",
    "                Data=tf.concat((Data,New_Data),axis=0)\n",
    "                New_Classification=tf.cast([Current_Classifications[random_i]],tf.string)\n",
    "                Classifications=tf.concat((Classifications,New_Classification),axis=0)\n",
    "                New_File=tf.cast([File_name],tf.string)\n",
    "                Files=tf.concat((Files,New_File),axis=0)\n",
    "                bar()\n",
    "            \n",
    "    Save_Image_Set(Data,\"External_Data_Image_Sets/\"+Final_Set_Name+\".npy\")   \n",
    "    Save_Image_Set(Classifications,\"External_Data_Image_Sets/\"+Final_Set_Name+\"_Classifications.npy\")\n",
    "    Save_Image_Set(Files,\"External_Data_Image_Sets/\"+Final_Set_Name+\"_Files.npy\")\n",
    "            \n",
    "    print(\"Current data set length: \",len(Data))\n",
    "    N=int(input(\"How many more images should be generated? \"))\n",
    "        \n",
    "    with alive_bar(N,force_tty=True) as bar:\n",
    "        for i in range(N):\n",
    "            if Classifications[-1]==\"Single Node\":\n",
    "                random_i=np.random.choice(Multiple_Node_Indices)\n",
    "            if Classifications[-1]==\"Multiple Nodes\":\n",
    "                random_i=np.random.choice(Single_Node_Indices)\n",
    "            File_name=Files[random_i]\n",
    "            alldata=Table.read(File_name.numpy().decode(\"utf-8\"))\n",
    "            xdata=alldata[\"pos_x\"]\n",
    "            ydata=alldata[\"pos_y\"]\n",
    "            zdata=alldata[\"pos_z\"]\n",
    "            points=np.empty((len(xdata),3))\n",
    "            for j in range(len(xdata)):\n",
    "                points[j]=np.array([xdata[j],ydata[j],zdata[j]])\n",
    "            alpha,beta,gamma=random_uniform_array(-2*np.pi,2*np.pi,3)\n",
    "            projections=Projected_Points(points,alpha,beta,gamma)\n",
    "            New_Data=[Tensor_Image(projections)]\n",
    "            New_Data=tf.cast(New_Data,tf.float32)\n",
    "            Data=tf.concat((Data,New_Data),axis=0)\n",
    "            New_Classification=tf.cast([Current_Classifications[random_i]],tf.string)\n",
    "            Classifications=tf.concat((Classifications,New_Classification),axis=0)\n",
    "            New_File=tf.cast([File_name],tf.string)\n",
    "            Files=tf.concat((Files,New_File),axis=0)\n",
    "            bar()\n",
    "            \n",
    "    Save_Image_Set(Data,\"External_Data_Image_Sets/\"+Final_Set_Name+\".npy\")   \n",
    "    Save_Image_Set(Classifications,\"External_Data_Image_Sets/\"+Final_Set_Name+\"_Classifications.npy\")\n",
    "    Save_Image_Set(Files,\"External_Data_Image_Sets/\"+Final_Set_Name+\"_Files.npy\")\n",
    "     \n",
    "def Check_Set_Length(File_List,Set_Fraction):\n",
    "    Total=0\n",
    "    for File in File_List:\n",
    "        Data=Load_Array(File)\n",
    "        Data=Data[:(int(Set_Fraction*len(Data)))]\n",
    "        print(File,\":\",len(Data))\n",
    "        Total+=len(Data)\n",
    "    print(\"Total Set Length :\",Total)\n",
    "\n",
    "def Model_Creator(File_List,Validation_Fraction,Set_Fraction,epoch_number,class_names,classifier,save,Folder,Model_Name):\n",
    "    with alive_bar(int(len(File_List)),force_tty=True) as bar:\n",
    "        File_Name_0=File_List[0]\n",
    "        Classification_0=File_List[0][:-4]+\"_Classifications.npy\"\n",
    "        Data_Set=Load_Array(File_Name_0)\n",
    "        Data_Set=Data_Set[:(int(Set_Fraction*len(Data_Set)))]\n",
    "        Data_Classifications=Load_Array(Classification_0)\n",
    "        Data_Classifications=Data_Classifications[:(int(Set_Fraction*len(Data_Classifications)))]\n",
    "        Data_Labels=Classifications_to_Labels(Data_Classifications,class_names)\n",
    "        bar()\n",
    "    \n",
    "        for i in range(1,len(File_List)):\n",
    "            File_Name=File_List[i]\n",
    "            Classification=File_List[i][:-4]+\"_Classifications.npy\"\n",
    "            New_Data=Load_Array(File_Name)\n",
    "            New_Data=New_Data[:(int(Set_Fraction*len(New_Data)))]\n",
    "            New_Classifications=Load_Array(Classification)\n",
    "            New_Classifications=New_Classifications[:(int(Set_Fraction*len(New_Classifications)))]\n",
    "            New_Labels=Classifications_to_Labels(New_Classifications,class_names)\n",
    "        \n",
    "            Data_Set=tf.concat((Data_Set,New_Data),axis=0)\n",
    "            Data_Labels=tf.concat((Data_Labels,New_Labels),axis=0)\n",
    "            bar()\n",
    "        \n",
    "    indices = tf.range(start=0, limit=tf.shape(Data_Set)[0], dtype=tf.int32)\n",
    "    shuffled_indices = tf.random.shuffle(indices)\n",
    "    \n",
    "    Data_Set=tf.gather(Data_Set,shuffled_indices)\n",
    "    Data_Labels=tf.gather(Data_Labels,shuffled_indices)\n",
    "    \n",
    "    Data_Training=Data_Set[int(Validation_Fraction*len(Data_Set)):]\n",
    "    Data_Test=Data_Set[:int(Validation_Fraction*len(Data_Set))]\n",
    "    \n",
    "    Labels_Training=Data_Labels[int(Validation_Fraction*len(Data_Labels)):]\n",
    "    Labels_Test=Data_Labels[:int(Validation_Fraction*len(Data_Labels))]\n",
    "    \n",
    "    Data_shape=(Data_Training.shape[1],Data_Training.shape[2],Data_Training.shape[3])\n",
    "    Model=keras.Sequential()\n",
    "    Model.add(keras.layers.Conv2D(16,3,padding=\"same\",activation=\"relu\",input_shape=Data_shape))\n",
    "    Model.add(keras.layers.MaxPooling2D())\n",
    "    Model.add(keras.layers.Conv2D(32,3,padding=\"same\",activation=\"relu\"))\n",
    "    Model.add(keras.layers.MaxPooling2D())\n",
    "    Model.add(keras.layers.Conv2D(64,3,padding=\"same\",activation=\"relu\"))\n",
    "    Model.add(keras.layers.MaxPooling2D())\n",
    "    Model.add(keras.layers.Flatten())\n",
    "    Model.add(keras.layers.Dense(128,activation=\"relu\"))\n",
    "    Model.add(keras.layers.Dense(len(class_names)))\n",
    "    Model.compile(optimizer=classifier,\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=[\"accuracy\"])\n",
    "    Model.fit(Data_Training,Labels_Training,epochs=epoch_number,validation_data=(Data_Test,Labels_Test))\n",
    "    loss,accuracy=Model.evaluate(Data_Test,Labels_Test,verbose=2)\n",
    "    \n",
    "    if save:\n",
    "        Model.save(Folder+\"/\"+Model_Name)\n",
    "        \n",
    "def New_Image_Classifier(Data_Folder,Model_Folder,Model_Name,class_names,save,save_path,save_Folder):\n",
    "    Data_Folder=os.path.abspath(Data_Folder)\n",
    "    Model_Folder=os.path.abspath(Model_Folder)\n",
    "    save_path=os.path.abspath(save_path)\n",
    "    Model=tf.keras.models.load_model(Model_Folder+\"/\"+Model_Name,compile=True)\n",
    "    Files,Data=Image_Reader(Data_Folder)\n",
    "    Labels=np.argmax(Model.predict(Data),axis=1)\n",
    "    Classifications=np.empty(Labels.shape,object)\n",
    "    for i in range(len(Labels)):\n",
    "        Classifications[i]=class_names[Labels[i]]\n",
    "    Save_Data=np.array([\"Predictions Made by: \"+Model_Name,Files,Classifications])\n",
    "    \n",
    "    New_directory=str(save_path+\"/\"+save_Folder)\n",
    "    if os.path.isdir(New_directory)==False:\n",
    "        os.mkdir(New_directory)\n",
    "    \n",
    "    Text_File_Name=str(New_directory+\"/Predictions.txt\")\n",
    "    NPY_File_Name=str(New_directory+\"/Predictions.npy\")\n",
    "    np.savetxt(Text_File_Name,Save_Data,fmt=\"%s\")\n",
    "    np.save(NPY_File_Name,Save_Data)\n",
    "    \n",
    "def Large_Set_Image_Classifier(Data_Folder,Model_Folder,Model_Name,class_names,save,save_path,save_Folder):\n",
    "    Data_Folder=os.path.abspath(Data_Folder)\n",
    "    Model_Folder=os.path.abspath(Model_Folder)\n",
    "    save_path=os.path.abspath(save_path)\n",
    "    Model=tf.keras.models.load_model(Model_Folder+\"/\"+Model_Name,compile=True)\n",
    "    cwd=os.getcwd()\n",
    "    os.chdir(Data_Folder)\n",
    "    Files=glob.glob(\"*.fits\")\n",
    "    New_directory=str(save_path+\"/\"+save_Folder)\n",
    "    if os.path.isdir(New_directory)==False:\n",
    "        os.mkdir(New_directory)\n",
    "    for File in Files:\n",
    "        File_Info_Folder=New_directory+\"/\"+str(File)\n",
    "        if os.path.isdir(File_Info_Folder)==False:\n",
    "            os.mkdir(File_Info_Folder)\n",
    "            alldata=Table.read(File)\n",
    "            xdata=alldata[\"pos_x\"]\n",
    "            ydata=alldata[\"pos_y\"]\n",
    "            zdata=alldata[\"pos_z\"]\n",
    "            points=np.empty((len(xdata),3))\n",
    "            for j in range(len(xdata)):\n",
    "                points[j]=np.array([xdata[j],ydata[j],zdata[j]])\n",
    "            Data=[Tensor_Image(points)]\n",
    "            Data=np.array(Data)\n",
    "            np.save(File_Info_Folder+\"/Tensor_Image.npy\",Data)\n",
    "            Label=np.argmax(Model.predict(Data))\n",
    "            Classification=np.array([class_names[Label]])\n",
    "            np.savetxt(File_Info_Folder+\"/Classification.txt\",Classification,fmt=\"%s\")\n",
    "            np.save(File_Info_Folder+\"/Classification.npy\",Classification) \n",
    "    os.chdir(cwd)\n",
    "    \n",
    "def Export_Results(Prediction_Folder,save_path,save_Folder):\n",
    "    cwd=os.getcwd()\n",
    "    Files,Tensor_Images,Classifications=Large_Set_Reader(Prediction_Folder)\n",
    "    Text_File_Data=np.column_stack([Files,Classifications])\n",
    "    NPY_File_Data=np.array([Files,Classifications],dtype=\"object\")\n",
    "    \n",
    "    New_directory=str(save_path+\"/\"+save_Folder)\n",
    "    if os.path.isdir(New_directory)==False:\n",
    "        os.mkdir(New_directory)\n",
    "    os.chdir(New_directory)\n",
    "    \n",
    "    np.savetxt(\"Results.txt\",Text_File_Data,delimiter=\",\",fmt=\"%s\")\n",
    "    np.save(\"Results.npy\",NPY_File_Data)\n",
    "    np.save(\"Images.npy\",Tensor_Images)\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "        \n",
    "def Accuracy_Metrics(File_List,Validation_Fraction,Set_Fraction,class_names,Model_Folder,Model_Name):\n",
    "    Model=tf.keras.models.load_model(Model_Folder+\"/\"+Model_Name,compile=True)\n",
    "    print(\"Loading Data Files\")\n",
    "    with alive_bar(int(len(File_List)),force_tty=True) as bar:\n",
    "        File_Name_0=File_List[0]\n",
    "        Classification_0=File_List[0][:-4]+\"_Classifications.npy\"\n",
    "        Data_Set=Load_Array(File_Name_0)\n",
    "        Data_Set=Data_Set[:(int(Set_Fraction*len(Data_Set)))]\n",
    "        Data_Classifications=Load_Array(Classification_0)\n",
    "        Data_Classifications=Data_Classifications[:(int(Set_Fraction*len(Data_Classifications)))]\n",
    "        Data_Labels=Classifications_to_Labels(Data_Classifications,class_names)\n",
    "        bar()\n",
    "    \n",
    "        for i in range(1,len(File_List)):\n",
    "            File_Name=File_List[i]\n",
    "            Classification=File_List[i][:-4]+\"_Classifications.npy\"\n",
    "            New_Data=Load_Array(File_Name)\n",
    "            New_Data=New_Data[:(int(Set_Fraction*len(New_Data)))]\n",
    "            New_Classifications=Load_Array(Classification)\n",
    "            New_Classifications=New_Classifications[:(int(Set_Fraction*len(New_Classifications)))]\n",
    "            New_Labels=Classifications_to_Labels(New_Classifications,class_names)\n",
    "        \n",
    "            Data_Set=tf.concat((Data_Set,New_Data),axis=0)\n",
    "            Data_Labels=tf.concat((Data_Labels,New_Labels),axis=0)\n",
    "            bar()\n",
    "        \n",
    "        \n",
    "    indices = tf.range(start=0, limit=tf.shape(Data_Set)[0], dtype=tf.int32)\n",
    "    shuffled_indices = tf.random.shuffle(indices)\n",
    "    \n",
    "    Data_Set=tf.gather(Data_Set,shuffled_indices)\n",
    "    Data_Labels=tf.gather(Data_Labels,shuffled_indices)\n",
    "    \n",
    "    Data_Test=Data_Set[:int(Validation_Fraction*len(Data_Set))]\n",
    "    Labels_Test=Data_Labels[:int(Validation_Fraction*len(Data_Labels))]\n",
    "    \n",
    "    Prediction_Labels=np.argmax(Model.predict(Data_Test),axis=1)\n",
    "\n",
    "    TP=0\n",
    "    FP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    print(\"Evaluating Data Set\")\n",
    "    with alive_bar(int(len(Prediction_Labels)),force_tty=True) as bar:\n",
    "        for i in range(len(Prediction_Labels)):\n",
    "            if Prediction_Labels[i]==0 and Labels_Test[i]==0:\n",
    "                TP+=1\n",
    "            if Prediction_Labels[i]==0 and Labels_Test[i]==1:\n",
    "                FP+=1\n",
    "            if Prediction_Labels[i]==1 and Labels_Test[i]==1:\n",
    "                TN+=1\n",
    "            if Prediction_Labels[i]==1 and Labels_Test[i]==0:\n",
    "                FN+=1\n",
    "            bar()\n",
    "        \n",
    "    p0=TP/(TP+FP)\n",
    "    r0=TP/(TP+FN)\n",
    "    F1_0=2*(p0*r0)/(p0+r0)\n",
    "    print(\"For Class 0:\")\n",
    "    print(\"Precision: (worst:0,best:1)\",p0)\n",
    "    print(\"Recall: (worst:0,best:1)\",r0)\n",
    "    print(\"F1 score: (worst:0,best:1)\",F1_0)\n",
    "    print(\"\")\n",
    "    p1=TN/(TN+FN)\n",
    "    r1=TN/(TN+FP)\n",
    "    F1_1=2*(p1*r1)/(p1+r1)\n",
    "    print(\"For Class 1:\")\n",
    "    print(\"Precision: (worst:0,best:1)\",p1)\n",
    "    print(\"Recall:(worst:0,best:1)\",r1)\n",
    "    print(\"F1 score: (worst:0,best:1)\",F1_1)\n",
    "    print(\"\")\n",
    "    print(\"Independent of Class:\")\n",
    "    ACC=(TP+TN)/(TP+FP+TN+FN)\n",
    "    print(\"Accuracy: (worst:0,best:1)\",ACC)\n",
    "    MCC=((TP*TN)-(FP*FN))/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "    print(\"Matthews Correlation Coefficient: (worst:-1,best:1)\",MCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Set Creator\n",
    "\n",
    "Use the following code to create and save data sets consisting of images and classifications of external data. The classification information is imported from a public google spreadsheet (must have share settings turned to \"anyone with the link\") with columns denoted \"Halo ID\" and \"Classification\". Visit https://docs.google.com/spreadsheets/d/1kW7Veg-SqdSNLBt9pQYPB3_xrPDxJOiVp-jpcQK2_s8/edit#gid=1114089073 to see an example of the required format. The variable \"Folder\" refers to the folder in which the input data is stored in the form of .fits files with names including the Halo ID; \"Sheet_ID\" refers to a unique string found within the URL of the desired google sheet (for this example, Sheet_ID=\"1kW7Veg-SqdSNLBt9pQYPB3_xrPDxJOiVp-jpcQK2_s8\"); \"Sheet_Names\" refers to a list of the names of the sheets in the instance that there are multiple located at this google address (use the \"Sheet_Names\" function in the example spreadsheet appscript to create this list more easily); \"Set_Name\" refers to the name of the resulting .npy files that are created to store input images, classifications, and corresponding file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:30:35.637854Z",
     "start_time": "2022-04-27T08:30:25.726305Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 14/14 [100%] in 7.9s (1.76/s)                           ▆█▆ 4/14 [29%] in 3s (1.3/s, eta: 8s)            \n"
     ]
    }
   ],
   "source": [
    "Folder=\"Example_Fits_Files\"\n",
    "Sheet_ID=\"1kW7Veg-SqdSNLBt9pQYPB3_xrPDxJOiVp-jpcQK2_s8\"\n",
    "Sheet_Names=['1', '7', '10', '23', '25' ]\n",
    "Set_Name=\"Example\"\n",
    "\n",
    "Sheet_Data_Set_Creator(Folder,Sheet_ID,Sheet_Names,Set_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cases in which classifications are not known, these Halo IDs can be given the label \"nan\" and combined into a data set using the following function. All of the input variables are the same as the previous function and provides a similar output with the aim of using this data to obtain new classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nan Set Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:30:40.506794Z",
     "start_time": "2022-04-27T08:30:37.396123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 2/2 [100%] in 2.0s (0.99/s)                      \n"
     ]
    }
   ],
   "source": [
    "Folder=\"Example_Fits_Files\"\n",
    "Sheet_ID=\"1kW7Veg-SqdSNLBt9pQYPB3_xrPDxJOiVp-jpcQK2_s8\"\n",
    "Sheet_Names=['1', '7', '10' ]\n",
    "Set_Name=\"nan_Example\"\n",
    "\n",
    "nan_Set_Creator(Folder,Sheet_ID,Sheet_Names,Set_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is used to lengthen the dataset and ensure that each class has approximately an equal number of entries. This is the final step for preparing a data set for model creation. The variables \"Current_Set_Name\" and \"Final_Set_Name\" refer to the name of the saved dataset file and that of the output file respectively (note that these names can be the same if desired). Once this function begins running, it will indicate the current length of the dataset and then ask the user for how many new images should be generated and added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:31:22.452652Z",
     "start_time": "2022-04-27T08:30:45.977538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 7/7 [100%] in 10.1s (0.69/s)                     1/7 [14%] in 2s (0.4/s, eta: 9s)                                    \n",
      "Current data set length:  21\n",
      "How many more images should be generated? 19\n",
      "|████████████████████████████████████████| 19/19 [100%] in 22.8s (0.83/s)                   \n"
     ]
    }
   ],
   "source": [
    "Current_Set_Name=\"Example\"\n",
    "Final_Set_Name=\"Extended_Example\"\n",
    "\n",
    "Data_Set_Extender(Current_Set_Name,Final_Set_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Creator\n",
    "\n",
    "The following code is used to check the length of the input data before they are used to train the model. Because there is an approximate maximum of 200000 total images due to memory limitations, a specified portion can be selected as to not exceed this value. \"File_List\" refers to the selection of dataset files used to train the model; \"Set_Fraction\" refers to the amount of the total files that will be used to determine the length of the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:31:26.169517Z",
     "start_time": "2022-04-27T08:31:26.159952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External_Data_Image_Sets/Extended_Example.npy : 40\n",
      "Total Set Length : 40\n"
     ]
    }
   ],
   "source": [
    "File_List=[\"External_Data_Image_Sets/Extended_Example.npy\"]\n",
    "Set_Fraction=1\n",
    "\n",
    "Check_Set_Length(File_List,Set_Fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to create and train an image classification model. This section implements an entirely new set of variables. \"Validation_Fraction\" refers to the percentage of total data assigned to test the accuracy of the final model; \"epoch_number\" refers to the number of epochs that will occur during the model training; \"class_names\" represents the list of total classifications present (in this case, it will always remain \"Single Node\" and \"Multiple Nodes\"); \"classifier\" refers the algorithm used for training the model (see https://www.tensorflow.org/api_docs/python/tf/keras/optimizers and look under \"Classes\" for more options); \"save\" represents the option to save the model that has just been created, set \"True\" to record the trained model or \"False\" if you do not want it to be saved; \"Folder\" refers to the directory to which the model can be saved (this folder is externally created); and Model_Name refers to the name given to the model if it is saved (It will appear as a new folder in the directory chosen for saved models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:31:35.122398Z",
     "start_time": "2022-04-27T08:31:29.907556Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 1/1 [100%] in 0.1s (9.72/s)                      \n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 925ms/step - loss: 0.6694 - accuracy: 0.6562 - val_loss: 0.6936 - val_accuracy: 0.6250\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6623 - accuracy: 0.6562 - val_loss: 0.6940 - val_accuracy: 0.6250\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6578 - accuracy: 0.6250 - val_loss: 0.6942 - val_accuracy: 0.6250\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6538 - accuracy: 0.6250 - val_loss: 0.6935 - val_accuracy: 0.6250\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6503 - accuracy: 0.6250 - val_loss: 0.6936 - val_accuracy: 0.6250\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6469 - accuracy: 0.6250 - val_loss: 0.6941 - val_accuracy: 0.6250\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6440 - accuracy: 0.6250 - val_loss: 0.6944 - val_accuracy: 0.6250\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6414 - accuracy: 0.6250 - val_loss: 0.6946 - val_accuracy: 0.6250\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6387 - accuracy: 0.6250 - val_loss: 0.6950 - val_accuracy: 0.6250\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6360 - accuracy: 0.6562 - val_loss: 0.6952 - val_accuracy: 0.6250\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6332 - accuracy: 0.6562 - val_loss: 0.6962 - val_accuracy: 0.6250\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6300 - accuracy: 0.6562 - val_loss: 0.6971 - val_accuracy: 0.6250\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6269 - accuracy: 0.6562 - val_loss: 0.6962 - val_accuracy: 0.6250\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6237 - accuracy: 0.6562 - val_loss: 0.6969 - val_accuracy: 0.6250\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6206 - accuracy: 0.6562 - val_loss: 0.6958 - val_accuracy: 0.6250\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6178 - accuracy: 0.6875 - val_loss: 0.6965 - val_accuracy: 0.6250\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6150 - accuracy: 0.6875 - val_loss: 0.6952 - val_accuracy: 0.6250\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6125 - accuracy: 0.6875 - val_loss: 0.6954 - val_accuracy: 0.6250\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6097 - accuracy: 0.6875 - val_loss: 0.6955 - val_accuracy: 0.6250\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6072 - accuracy: 0.6875 - val_loss: 0.6953 - val_accuracy: 0.6250\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6047 - accuracy: 0.6875 - val_loss: 0.6963 - val_accuracy: 0.6250\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6022 - accuracy: 0.6875 - val_loss: 0.6953 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5996 - accuracy: 0.6875 - val_loss: 0.6972 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5970 - accuracy: 0.6875 - val_loss: 0.6957 - val_accuracy: 0.3750\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5944 - accuracy: 0.7188 - val_loss: 0.6970 - val_accuracy: 0.3750\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5919 - accuracy: 0.6875 - val_loss: 0.6956 - val_accuracy: 0.3750\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5895 - accuracy: 0.7500 - val_loss: 0.6970 - val_accuracy: 0.3750\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5870 - accuracy: 0.7188 - val_loss: 0.6959 - val_accuracy: 0.3750\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5845 - accuracy: 0.7500 - val_loss: 0.6966 - val_accuracy: 0.3750\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5820 - accuracy: 0.7500 - val_loss: 0.6960 - val_accuracy: 0.3750\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5797 - accuracy: 0.7500 - val_loss: 0.6971 - val_accuracy: 0.3750\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5772 - accuracy: 0.7500 - val_loss: 0.6955 - val_accuracy: 0.3750\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5747 - accuracy: 0.7500 - val_loss: 0.6970 - val_accuracy: 0.3750\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5721 - accuracy: 0.7500 - val_loss: 0.6962 - val_accuracy: 0.3750\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5697 - accuracy: 0.7188 - val_loss: 0.6968 - val_accuracy: 0.3750\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5671 - accuracy: 0.7812 - val_loss: 0.6972 - val_accuracy: 0.3750\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5645 - accuracy: 0.7500 - val_loss: 0.6973 - val_accuracy: 0.3750\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5620 - accuracy: 0.7812 - val_loss: 0.6977 - val_accuracy: 0.3750\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5594 - accuracy: 0.7500 - val_loss: 0.6986 - val_accuracy: 0.3750\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5568 - accuracy: 0.7812 - val_loss: 0.6988 - val_accuracy: 0.3750\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5543 - accuracy: 0.7812 - val_loss: 0.6988 - val_accuracy: 0.3750\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5517 - accuracy: 0.7812 - val_loss: 0.7000 - val_accuracy: 0.3750\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5490 - accuracy: 0.7812 - val_loss: 0.7007 - val_accuracy: 0.3750\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5462 - accuracy: 0.7812 - val_loss: 0.7008 - val_accuracy: 0.3750\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5436 - accuracy: 0.7812 - val_loss: 0.7021 - val_accuracy: 0.3750\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5407 - accuracy: 0.7812 - val_loss: 0.7027 - val_accuracy: 0.3750\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5378 - accuracy: 0.7812 - val_loss: 0.7034 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5351 - accuracy: 0.8125 - val_loss: 0.7037 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5324 - accuracy: 0.7812 - val_loss: 0.7050 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5295 - accuracy: 0.8125 - val_loss: 0.7064 - val_accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.7064 - accuracy: 0.5000\n",
      "INFO:tensorflow:Assets written to: Saved_Models/Example_Model/assets\n"
     ]
    }
   ],
   "source": [
    "File_List=[\"External_Data_Image_Sets/Extended_Example.npy\"]\n",
    "Validation_Fraction=0.2\n",
    "Set_Fraction=1\n",
    "epoch_number=50\n",
    "class_names=[\"Single Node\",\"Multiple Nodes\"]\n",
    "classifier=\"SGD\"\n",
    "save=True\n",
    "Folder=\"Saved_Models\"\n",
    "Model_Name=\"Example_Model\"\n",
    "\n",
    "Model_Creator(File_List,Validation_Fraction,Set_Fraction,epoch_number,class_names,classifier,save,Folder,Model_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to save the inputs used to create the model. In this block, only the information of \"Loss\" and \"Accuracy\" will need to be changed. These values will be printed when the model training has completed. The rest of the variables will be the same as those defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:31:56.612553Z",
     "start_time": "2022-04-27T08:31:56.577694Z"
    }
   },
   "outputs": [],
   "source": [
    "Loss,Accuracy=0.7064, 0.5000\n",
    "\n",
    "Summary=[\"Files\",File_List,\n",
    "         \"Validation_Fraction=\",Validation_Fraction,\n",
    "         \"Set_Fraction=\",Set_Fraction,\n",
    "         \"epoch_number=\",epoch_number,\n",
    "         \"classifier=\",classifier,\n",
    "         \"Loss=\",Loss,\n",
    "         \"Accuracy=\",Accuracy]\n",
    "\n",
    "\n",
    "New_directory=str(Folder+\"/\"+Model_Name+\"_Parameters/\")\n",
    "if os.path.isdir(New_directory)==False:\n",
    "        os.mkdir(New_directory)\n",
    "    \n",
    "Textfile_Data=np.array(Summary,dtype=object)\n",
    "Name=str(New_directory+Model_Name+\"_Overview.txt\")\n",
    "np.savetxt(Name,Textfile_Data,fmt=\"%s\")\n",
    "NPY_Name=str(New_directory+Model_Name+\"_Overview.npy\")\n",
    "np.save(NPY_Name,Textfile_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. New Data Predictions\n",
    "\n",
    "In running this code, fits files from other sources are evaluated by a model and given either the classification of \"Single Node\" or \"Multiple Nodes.\" Important data will be saved as a .txt file for easy viewing as well as a .npy file to be easily loaded into python notebooks. The final block also introduces new variables used to specify the original folder containing the fits files to be evaluated and the final location for saving the predictions that the model makes. More specifically, \"Data_Folder\" refers to the folder of input data / externally obtained fits files (Note: every file ending in .fits contained in the provided folder will be evaluated); \"Model_Folder\" refers to the directory that contains the model selected to make the evaluations; \"save_path\" refers to the directory in which saved evaluations will be stored; \"save_Folder\" refers to the name given to the folder storing the .txt and .npy outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:32:03.603665Z",
     "start_time": "2022-04-27T08:32:00.724084Z"
    }
   },
   "outputs": [],
   "source": [
    "Data_Folder=\"New_Data\"\n",
    "Model_Folder=\"Saved_Models\"\n",
    "Model_Name=\"Final_Model\"\n",
    "class_names=[\"Single Node\",\"Multiple Nodes\"]\n",
    "save=True\n",
    "save_path=\"New_Image_Predictions\"\n",
    "save_Folder=\"Example_Predictions[Final_Model]\"\n",
    "\n",
    "New_Image_Classifier(Data_Folder,Model_Folder,Model_Name,class_names,save,save_path,save_Folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code serves the same purpose as the previous function, however this block is better suited to evaluating extremely large datasets. The input variables are all the same, but the output format is very different. Each image evaluation is given its own subdirectory in which a .txt and .npy output is stored. If the process is interrupted and restarted, the previously completed evaluations will be recognized and the process can resume by running on new data. Check that the most recently created subdirectory contains the output data before rerunning. If this is not the case, delete this subdirectory before resuming the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:32:07.024591Z",
     "start_time": "2022-04-27T08:32:06.344404Z"
    }
   },
   "outputs": [],
   "source": [
    "Data_Folder=\"New_Data\"\n",
    "Model_Folder=\"Saved_Models\"\n",
    "Model_Name=\"Final_Model\"\n",
    "class_names=[\"Single Node\",\"Multiple Nodes\"]\n",
    "save=True\n",
    "save_path=\"New_Image_Predictions\"\n",
    "save_Folder=\"Example_Long_Set_Predictions[Final_Model]\"\n",
    "\n",
    "Large_Set_Image_Classifier(Data_Folder,Model_Folder,Model_Name,class_names,save,save_path,save_Folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the \"Large_Set_Classifier\" is used as opposed to the normal function, the following code can be used to condense all of the output data into a single location for better interpretation. The \"Prediction_Folder\" variable should be the same as the \"save_Folder\" variable used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:32:13.658704Z",
     "start_time": "2022-04-27T08:32:13.359705Z"
    }
   },
   "outputs": [],
   "source": [
    "Prediction_Folder=\"New_Image_Predictions/Example_Long_Set_Predictions[Final_Model]\"\n",
    "save_path=\"New_Image_Predictions\"\n",
    "save_Folder=\"Example_Long_Set_Complete_Predictions\"\n",
    "\n",
    "Export_Results(Prediction_Folder,save_path,save_Folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In running this code, several accuracy metrics of the trained model can be calculated and printed by selecting the same parameters used in the model's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T08:32:19.875371Z",
     "start_time": "2022-04-27T08:32:18.863682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Files\n",
      "|████████████████████████████████████████| 1/1 [100%] in 0.1s (9.81/s)                      \n",
      "Evaluating Data Set\n",
      "|████████████████████████████████████████| 8/8 [100%] in 0.1s (78.03/s)                     \n",
      "For Class 0:\n",
      "Precision: (worst:0,best:1) 1.0\n",
      "Recall: (worst:0,best:1) 0.6666666666666666\n",
      "F1 score: (worst:0,best:1) 0.8\n",
      "\n",
      "For Class 1:\n",
      "Precision: (worst:0,best:1) 0.8333333333333334\n",
      "Recall:(worst:0,best:1) 1.0\n",
      "F1 score: (worst:0,best:1) 0.9090909090909091\n",
      "\n",
      "Independent of Class:\n",
      "Accuracy: (worst:0,best:1) 0.875\n",
      "Matthews Correlation Coefficient: (worst:-1,best:1) 0.7453559924999299\n"
     ]
    }
   ],
   "source": [
    "File_List=[\"External_Data_Image_Sets/Extended_Example.npy\"]\n",
    "Validation_Fraction=0.2\n",
    "Set_Fraction=1\n",
    "class_names=[\"Single Node\",\"Multiple Nodes\"]\n",
    "Folder=\"Saved_Models\"\n",
    "Model_Name=\"Final_Model\"\n",
    "\n",
    "Accuracy_Metrics(File_List,Validation_Fraction,Set_Fraction,class_names,Folder,Model_Name)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-Python_3] *",
   "language": "python",
   "name": "conda-env-.conda-Python_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "96px",
    "width": "430px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
